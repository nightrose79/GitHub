{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow and Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "project templates:\n",
    "\n",
    "* https://github.com/pavopax/r-workshop-odsc/tree/master/templates/sample-directory-structure\n",
    "* http://projecttemplate.net/architecture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tidyverse 1.0: all the packages you need, all in one package\n",
    "\n",
    "https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronicity\n",
    "\n",
    "A **future** is a promise that the result of a computation will be made available at some point in the future. If you query a future in a resolved state, the value is made available. But if you query a future in an unresolved state, you have to wait for the computation to finish. This is called being *blocked*.\n",
    "\n",
    "The benefit of futures is that your code is no longer limited to sequential operation. Futures can be resolved *lazily* (only when requested), *eagerly* (as soon as it's created), or *asynchronously* (using parallel processing). And when you declare a future, the rest of your code can proceed while you wait for the computation to finish. It's even possible to check the status of a future in a non-blocking way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/HenrikBengtsson/future\n",
    "library(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implicitly create a future\n",
    "a %<-% { 2 + 2 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f <- future({ expr })`\n",
    "\n",
    "`v <- value(f)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value(future({2 + 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: How would a for loop of futures work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallelism\n",
    "\n",
    "Hardware requirements:\n",
    "* Threads\n",
    "* Cores\n",
    "* GPUs\n",
    "* Clusters\n",
    "* Cloud\n",
    "\n",
    "Software requirements:\n",
    "* Tasks\n",
    "* Libraries\n",
    "* Algorithms\n",
    "\n",
    "We'll focus today on paralellizing tasks and distributed computation libraries using multiple cores and some simple local clusters.\n",
    "\n",
    "For distributed algorithm information check the docs, for example SparkR has open sourced exactly how they parallelize model training.\n",
    "\n",
    "For utilizing GPUs, check out the package `gputools`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: Can you name an embarrassingly parallel problem? What about a problem difficult to paralellize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(glm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df <- iris\n",
    "str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features <- c('Sepal.Length','Sepal.Width','Petal.Length','Petal.Width')\n",
    "label <- 'Species'\n",
    "classes <- df[[label]] %>% unique() %>% sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitBinomialClassifier <- function(label, class, features, data) {\n",
    "  formula <- paste('(', label,'==\"', class,'\") ~ ',\n",
    "                   paste(features, collapse=' + '), sep='')\n",
    "  print(formula)\n",
    "  glm(as.formula(formula), family = binomial, data = data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system.time(\n",
    "for(class in classes) {\n",
    "  print(\"*****\")\n",
    "  print(class)\n",
    "  print(fitBinomialClassifier(label, class, features, df))\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine, but we don't need to train each classifier one at a time. Just as we replaced iteration with vectorized transforms in data frames, we can replace this for loop with a map-type operation called `lapply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worker <- function(class) {  # A curried function\n",
    "  fitBinomialClassifier(label, class, features, df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system.time({\n",
    "models <- lapply(classes, worker)  # map(seq, f)\n",
    "names(models) <- classes\n",
    "print(models)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[12:17,] %>%\n",
    "  select(Petal.Width) %>%\n",
    "  lapply(function(x) c(-x, pi*(x/2)^2)) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*: Use a larger dataset and `system.time` to measure the performance improvement, if any, of using `lapply`. Does it vary by the complexity of the calulation? What about the number of observations? What about the number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel\n",
    "\n",
    "[Documentation](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores <- detectCores() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parallelCluster <- makeCluster(cores)\n",
    "print(parallelCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parLapply(parallelCluster,\n",
    "          df$Sepal.Width,\n",
    "          function(x) pi*(x/2)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tryCatch(\n",
    "  models <- parLapply(parallelCluster,\n",
    "                      classes, worker),\n",
    "  error = function(e) print(e)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Libraries must be defined on each remote machine\n",
    "* Prefer package-style notation such as stats::glm() when calling library functions (as calling library(...) on each remote node would be wasteful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base <- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parLapply(parallelCluster, \n",
    "          2:4, \n",
    "          function(exponent) base^exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterExport(parallelCluster, \"base\")\n",
    "base <- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, `clusterEvalQ(cluster, library(<package>)` can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopCluster(parallelCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parallelCluster <- makeCluster(detectCores()-1, type = \"FORK\")\n",
    "print(parallelCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the single argument function we are going to pass to parallel\n",
    "mkWorker <- function(class, features, df) {\n",
    "  # make sure each of the three values we need passed \n",
    "  # are available in this environment\n",
    "  force(class)\n",
    "  force(features)\n",
    "  force(df)\n",
    "  # define any and every function our worker function \n",
    "  # needs in this environment\n",
    "  fitBinomialClassifier <- function(label, class, features, data) {\n",
    "    formula <- paste('(', label, '==\"', class, '\") ~ ',\n",
    "                     paste(features, collapse=' + '), sep='')\n",
    "    glm(as.formula(formula), family = binomial, data = data)\n",
    "  }\n",
    "  # Finally: define and return our worker function.\n",
    "  # The function worker's \"lexical closure\" \n",
    "  # (where it looks for unbound variables)\n",
    "  # is mkWorker's activation/execution environment \n",
    "  # and not the usual Global environment.\n",
    "  # The parallel library is willing to transport \n",
    "  # this environment (which it does not\n",
    "  # do for the Global environment).\n",
    "  worker <- function(class) {\n",
    "    fitBinomialClassifier(label, class, features, df)\n",
    "  }\n",
    "  return(worker)\n",
    "}\n",
    "\n",
    "system.time({\n",
    "models <- parLapply(parallelCluster, classes,\n",
    "                    mkWorker(class, features, df))\n",
    "names(models) <- classes\n",
    "print(models)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meet the whole family:\n",
    "* `parSapply` (`sapply` is a simple wrapper around `lapply`)\n",
    "* `parLapplyLB` and `parSapplyLB` include load balancing\n",
    "* `parApply`, `parRapply`, and `parCapply` for matrices, rows, and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parSapply(parallelCluster, as.character(2:4), \n",
    "          function(exponent){\n",
    "            x <- as.numeric(exponent)\n",
    "            c(base = base^x, self = x^x)\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A safer way to stop the cluster\n",
    "if(!is.null(parallelCluster)) {\n",
    "  stopCluster(parallelCluster)\n",
    "  parallelCluster <- c()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And futures as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "availableCores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl <- makeCluster(availableCores() - 1)\n",
    "plan(cluster, workers=cl)\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pid <- Sys.getpid()\n",
    "pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a %<-% {\n",
    "  cat(\"Calculating a...\\n\")\n",
    "  Sys.getpid()\n",
    "}\n",
    "\n",
    "b %<-% {\n",
    "  rm(pid)\n",
    "  cat(\"Calculating b...\\n\")\n",
    "  Sys.getpid()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat(a, b, pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Works on all OS\n",
    "plan(multisession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Won't work on Windows, the multiprocess plan defaults to this\n",
    "plan(multicore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plan(lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreach\n",
    "\n",
    "Another popular package worth mentioning that uses `doParallel`. It lies somewhere between a `for` loop and `lapply`. [Docs](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "It's often useful and performant to cache results to prevent having to rerun calculations. There are several useful libraries including `R.cache` and `DataCache`. This example just does it manually using `digest` to hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl <- makeCluster(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cacheParallel <- function() {\n",
    "  vars <- 1:2\n",
    "  tmp <- clusterEvalQ(cl, \n",
    "                      library(digest))\n",
    " \n",
    "  parSapply(cl, vars, function(var) {\n",
    "    fn <- function(x) x^2\n",
    "    dg <- digest(list(fn, var))\n",
    "    cache_fn <- \n",
    "      sprintf(\"Cache_%s.Rdata\", dg)\n",
    "    if (file.exists(cache_fn)) {\n",
    "      load(cache_fn)\n",
    "    } else {\n",
    "      var <- fn(var); \n",
    "      Sys.sleep(5)\n",
    "      save(var, file = cache_fn)\n",
    "    }\n",
    "    return(var)\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system.time(out <- cacheParallel())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system.time(out <- cacheParallel())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "file.remove(list.files(pattern = \"Cache.+.Rdata\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopCluster(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: How will caching impact the performance of eg. `parLapply` above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Memory is often *the* limiting factor in distributed jobs. Network bandwidth can make things slow but memory problems kill jobs. Using `FORK` if you can does help as it prevents having to define copies of variables in different closures. In addition:\n",
    "* Use rm() frequently to keep the environment clean\n",
    "* Call gc() to run the garbage collector\n",
    "* Limit the concurrent cores running based on the environment memory\n",
    "* Selectively apply parallelism to your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* `http://www.win-vector.com/blog/2016/01/parallel-computing-in-r/`\n",
    "* `https://www.r-bloggers.com/how-to-go-parallel-in-r-basics-tips/`\n",
    "* `https://github.com/HenrikBengtsson/future`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
